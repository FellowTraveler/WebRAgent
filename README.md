# ğŸ” WebRAgent

A Retrieval-Augmented Generation (RAG) web application built with Flask and Qdrant.

Â© 2024 Dennis Kruyt. All rights reserved.

## ğŸ“‹ Overview

This application implements a RAG system that combines the power of Large Language Models (LLMs) with a vector database (Qdrant) to provide context-enhanced responses to user queries. It features:

- ğŸ’¬ User query interface for asking questions
- ğŸ” Admin interface for managing document collections
- ğŸ“„ Document processing and embedding
- ğŸ¤– Integration with multiple LLM providers (OpenAI, Claude, Ollama)

## âœ¨ Features

- ğŸ–¥ï¸ **User Interface**: Clean, intuitive interface to submit queries and receive LLM responses
- ğŸŒ **Web Search**: Search the web directly using SearXNG integration with LLM result interpretation
- ğŸ¤– **Agent Search**: Break down complex questions into sub-queries for more comprehensive answers
- ğŸ§  **Mind Maps**: Visualize response concepts with automatically generated mind maps
- ğŸ” **Vector Search**: Retrieve relevant document snippets based on semantic similarity
- ğŸ‘¤ **Admin Interface**: Securely manage collections and upload documents
- ğŸ“ **Document Processing**: Automatically extract text, chunk, embed, and store documents
- ğŸ§  **Multiple LLM Support**: Configure your preferred LLM provider (OpenAI, Claude, Ollama)
- ğŸ” **Dynamic Embedding Models**: Automatically detects and uses available embedding models from all configured providers

## ğŸ“‹ Prerequisites

- ğŸ Python 3.8+
- ğŸ—„ï¸ [Qdrant](https://qdrant.tech/documentation/quick-start/) running locally or remotely
- ğŸ”‘ API keys for your chosen LLM provider

## ğŸš€ Installation

### ğŸ’» Option 1: Local Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/dkruyt/WebRAgent.git
   cd WebRAgent
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Copy the example environment file and configure it with your settings:
   ```bash
   cp .env.example .env
   ```

   Then edit the `.env` file with your preferred settings. Here are the key settings to configure:
   ```
   # API Keys for LLM Providers (uncomment and add your keys for the providers you want to use)
   # At least one provider should be configured
   #OPENAI_API_KEY=your_openai_api_key_here
   #CLAUDE_API_KEY=your_claude_api_key_here
   
   # Ollama Configuration (uncomment to use Ollama)
   #OLLAMA_HOST=http://localhost:11434
   
   # Qdrant Configuration
   QDRANT_HOST=localhost
   QDRANT_PORT=6333
   
   # SearXNG Configuration
   SEARXNG_URL=http://searxng:8080
   
   # Flask Secret Key (generate a secure random key for production)
   FLASK_SECRET_KEY=change_me_in_production
   
   # Admin User Configuration
   ADMIN_USERNAME=admin
   ADMIN_PASSWORD=change_me_in_production
   ```

   Note: The system will automatically detect and use models from the providers you've configured:
   
   - If you set OPENAI_API_KEY, it will use OpenAI models for both LLM and embeddings
   - If you set CLAUDE_API_KEY, it will use Claude models for LLM
   - If you set OLLAMA_HOST, it will use Ollama models for both LLM and embeddings
   - Sentence Transformers will be used as fallback embedding models
   
   There's no need to manually specify which models to use - the system dynamically detects available models.

5. Make sure you have Qdrant running locally or specify a remote instance in the `.env` file.

6. If using Ollama, make sure it's running locally or specify a remote instance in the `.env` file.

7. Start the application:
   ```bash
   python run.py
   ```

8. Access the application at `http://localhost:5000`

### ğŸ³ Option 2: Docker Installation

This project includes Docker and Docker Compose configurations for easy deployment.

1. Clone the repository:
   ```bash
   git clone https://github.com/dkruyt/WebRAgent.git
   cd WebRAgent
   ```

2. Start the application with Docker Compose:
   ```bash
   docker-compose up -d
   ```

3. The following services will be available:
   - ğŸŒ RAG Web Application: http://localhost:5000
   - ğŸ“Š Qdrant Dashboard: http://localhost:6333/dashboard
   - ğŸ” SearXNG Search Engine: http://localhost:8080

4. To shut down the application:
   ```bash
   docker-compose down
   ```

### ğŸ“¥ Pre-downloading Ollama Models

If you want to pre-download the Ollama models before starting the application:

```bash
# For main LLM models
ollama pull llama2
ollama pull mistral
ollama pull gemma

# For embedding models
ollama pull nomic-embed-text
ollama pull all-minilm
```

The system will automatically detect these models if they're available in your Ollama installation.

## ğŸ“– Usage

### ğŸ” User Interface
1. Navigate to the home page
2. Choose your search method:
   - **Document Search**: Select a collection from the dropdown
   - **Web Search**: Toggle the "Web Search" option
3. Enter your query in the text box
4. Configure additional options (optional):
   - **Generate Mind Map**: Toggle to visualize concepts related to your query
   - **Agent Search**: Enable for complex questions that benefit from being broken down
   - **Number of Results**: Adjust how many results to retrieve
5. Submit your query and view the response
6. Explore source documents or web sources that informed the answer

### ğŸŒ Web Search
1. Toggle the "Web Search" option on the main interface
2. Enter your query
3. The system will:
   - Search the web using SearXNG
   - Use an LLM to interpret and synthesize the search results
   - Present a comprehensive answer along with source links

### ğŸ¤– Agent Search
1. Enable the "Agent Search" checkbox
2. Choose a strategy:
   - **Direct Decomposition**: Breaks down your question into targeted sub-queries
   - **Informed Decomposition**: Gets initial results first, then creates follow-up queries
3. Submit your query to receive a comprehensive answer synthesized from multiple search operations

### ğŸ‘¤ Admin Interface
1. Login with admin credentials (default: username `admin`, password `admin123`)
2. Create new collections from the admin dashboard
3. Upload documents to collections
4. Documents are automatically processed and made available for retrieval

## ğŸ› ï¸ Technical Implementation

- ğŸŒ **Flask**: Web framework for the application
- ğŸ—„ï¸ **Qdrant**: Vector database for storing and retrieving document embeddings
- ğŸ” **SearXNG**: Self-hosted search engine for web search capabilities
- ğŸ¤– **Agent Framework**: Custom implementation for query decomposition and synthesis
- ğŸ§  **Mind Map Generation**: Visualization system for query responses
- ğŸ”¤ **Embedding Models**:
  - **SentenceTransformers**: Local embedding models (always available as fallback)
  - **OpenAI Embeddings**: High-quality embeddings when API key is configured
  - **Ollama Embeddings**: Local embedding models when Ollama is configured
- ğŸ”Œ **Model Management**: Dynamic provider detection and configuration based on available environment variables
- ğŸ” **Flask-Login**: For admin authentication
- ğŸ“š **Python Libraries**: For document processing (PyPDF2, BeautifulSoup, etc.)
- ğŸ“„ **Docling**: Advanced document processing capability for extracting text from various file formats

## ğŸ“‚ Project Structure

```
WebRAgent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/             # Data models
â”‚   â”‚   â”œâ”€â”€ chat.py         # Chat session models
â”‚   â”‚   â”œâ”€â”€ collection.py   # Document collection models
â”‚   â”‚   â”œâ”€â”€ document.py     # Document models and metadata
â”‚   â”‚   â””â”€â”€ user.py         # User authentication models
â”‚   â”œâ”€â”€ routes/             # Route handlers
â”‚   â”‚   â”œâ”€â”€ admin.py        # Admin interface routes
â”‚   â”‚   â”œâ”€â”€ auth.py         # Authentication routes
â”‚   â”‚   â”œâ”€â”€ chat.py         # Chat interface routes
â”‚   â”‚   â””â”€â”€ main.py         # Main application routes
â”‚   â”œâ”€â”€ services/           # Business logic
â”‚   â”‚   â”œâ”€â”€ agent_search_service.py     # Query decomposition and agent search
â”‚   â”‚   â”œâ”€â”€ chat_service.py             # Chat session management
â”‚   â”‚   â”œâ”€â”€ claude_service.py           # Anthropic Claude integration
â”‚   â”‚   â”œâ”€â”€ document_service.py         # Document processing
â”‚   â”‚   â”œâ”€â”€ llm_service.py              # LLM provider abstraction
â”‚   â”‚   â”œâ”€â”€ mindmap_service.py          # Mind map generation
â”‚   â”‚   â”œâ”€â”€ model_service.py            # Dynamic model management
â”‚   â”‚   â”œâ”€â”€ ollama_service.py           # Ollama integration
â”‚   â”‚   â”œâ”€â”€ openai_service.py           # OpenAI integration
â”‚   â”‚   â”œâ”€â”€ qdrant_service.py           # Vector database operations
â”‚   â”‚   â”œâ”€â”€ rag_service.py              # Core RAG functionality
â”‚   â”‚   â”œâ”€â”€ searxng_service.py          # Web search integration
â”‚   â”‚   â””â”€â”€ web_search_agent_service.py # Web search with agent capabilities
â”‚   â”œâ”€â”€ static/             # CSS, JS, and other static files
â”‚   â”œâ”€â”€ templates/          # Jinja2 templates
â”‚   â””â”€â”€ __init__.py         # Flask application factory
â”œâ”€â”€ data/                   # Created at runtime for data storage
â”‚   â”œâ”€â”€ collections/        # Collection metadata storage
â”‚   â”œâ”€â”€ documents/          # Document metadata storage
â”‚   â”œâ”€â”€ models/             # Model configuration storage
â”‚   â”‚   â”œâ”€â”€ config.json     # Dynamic model configuration
â”‚   â”‚   â””â”€â”€ dimensions.json # Embedding model dimensions
â”‚   â””â”€â”€ uploads/            # Uploaded document files
â”œâ”€â”€ searxng/                # SearXNG configuration
â”œâ”€â”€ .dockerignore           # Files to exclude from Docker build
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ .env.example            # Example environment file
â”œâ”€â”€ .gitignore              # Git ignore patterns
â”œâ”€â”€ docker-compose.yml      # Docker Compose config
â”œâ”€â”€ docker-compose.gpu.yml  # Docker Compose config with GPU support
â”œâ”€â”€ Dockerfile              # Docker build instructions
â”œâ”€â”€ requirements.txt        # Project dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ run.py                  # Application entry point
```

## ğŸ”’ Security Notes

- âš ï¸ This application uses a simple in-memory user store for demo purposes
- ğŸ›¡ï¸ In a production environment, use a proper database with password hashing
- ğŸ” Configure HTTPS for secure communication
- ğŸ”‘ Set a strong, unique `FLASK_SECRET_KEY`
- ğŸš« Do not expose admin routes to the public internet without proper security

## ğŸ“œ License

MIT

## ğŸ“ Copyright

Â© 2024 Dennis Kruyt. All rights reserved.

## ğŸ™ Acknowledgements

- ğŸ—„ï¸ [Qdrant](https://qdrant.tech/)
- ğŸ”¤ [SentenceTransformers](https://www.sbert.net/)
- ğŸŒ [Flask](https://flask.palletsprojects.com/)
- ğŸ“„ [Docling](https://github.com/doclingjs/docling)
- ğŸ” [SearXNG](https://docs.searxng.org/)
- ğŸ¤– [OpenAI](https://openai.com/)
- ğŸ§  [Anthropic](https://www.anthropic.com/)
- ğŸ¦™ [Ollama](https://ollama.ai/)