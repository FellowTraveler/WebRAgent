services:
  # RAG Flask application
  app:
    image: ghcr.io/dkruyt/webragent:latest
    ports:
      - "5000:5000"
    volumes:
      - ./data:/app/data
    depends_on:
      - qdrant
    #  - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
    restart: unless-stopped

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  # Ollama for LLM API (GPU version)
  #ollama:
  #  image: ollama/ollama:latest
  #  ports:
  #    - "11434:11434"
  #  volumes:
  #    - ollama_data:/root/.ollama
  #  restart: unless-stopped
  #  environment:
  #    - OLLAMA_MODEL=tinyllama
  #  command: >
  #    sh -c "ollama pull tinyllama nomic-embed-text && ollama serve"

volumes:
  qdrant_data:
#  ollama_data: